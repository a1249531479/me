

# 阅读报告

​       《BoimanShechtmanIrani_CVPR2008 - In Defense of Nearest-Neighbor Based Image Classification》一文强调了基于神经网络的非参数图像分类方法的有效性并介绍了一个基于nnn的分类器NBNN。作者对该分类器的原理与优点进行了分析并将其与其他基于学习的图像分类器进行了比较。

​         文章分为五部分

###      第一部分

​        介绍了图像分类方法的主要分类：

1.基于学习的方法分类器

这类分类器在该领域目前处于领先地位，特别是基于SVM的方法

2.非参数分类器

这类分类器直接根据数据进行决策，不需要学习。常见的有基于nnbase的分类器。



  介绍了非参数分类器的优缺点

优点

1. 可以自然地处理大量的类
2. 可避免参数过拟合
3. 不需要学习/训练阶段

缺点

与最先进的基于学习的方法之间性能差距巨大

分析：描述符量化和该方法只能在查询图像与数据库图像的相似时才能提供良好分类的特点所致

介绍NBNN;

特点：不需要描述符量化，使用直接的"图像到类"的距离，在贝叶斯假设下可准确逼近理论上最优的图像分类器。

### 原理：



![CSDN_1579444376630](C:\Users\1\Desktop\CSDN_1579444376630.jpg)

### 第二部分

标准的基于nnn 的图像分类器性能低下的原因

1.量化会损坏非参数分类器

由于粗略的量化，鉴别信息的数量大大减少了。基于学习的算法可以通过其学习阶段弥补部分信息损失，从而获得良好的分类结果。非参数算法没有“撤消”量化损害的训练阶段，就会被损坏。

高量化误差导致描述符的鉴别能力下降。此外，描述符的信息(区分性)越强，其区分能力的退化就越严重。

2.核方法的基础在标记(“训练”)图像数量较少的情况下，显著限制了非参数图像分类器的泛化能力。

n -image分类器不能泛化标记图像集之外的很多东西。

当对象形状和外观变化很大的类只有很少的标记图像时，就会得到错误的分类。

在泛化使用Image-to-Ckass距离时，分类器会得到更好的泛化能力

## 第三部分

.概率公式

 对MAP分类器最小化平均分类：*Cˆ = arg maxC p(C|Q). W

当类先验p(C)均匀时，MAP分类器退化为最大似然(ML)分类器:

![图片1](C:\Users\1\Desktop\图片1.png)

生成)概率模型，即Naive-Bayes假设(*


![图片4](C:\Users\1\Desktop\图片4.png)

*i=1取ML决策规则的log概率，得到:*

![图片5](C:\Users\1\Desktop\图片5.png)



## **NBNN算法:**

由于描述符分布的长尾特性，几乎所有的描述符在描述符空间中都是相当隔离的，因此与数据库中的大多数描述符相差甚远。因此，除少数项外，式(3)和式中的所有项都可以忽略不计(K随距离呈指数递减)。因此，我们可以精确地近似式(3)中使用(几个)r最大元素的总和。这些r个最大的元素与d类描述符d∈Q的r个近邻相对应，..*C*1 *d∈C:**L* *C*

![图片6](C:\Users\1\Desktop\图片6.png)



由此得到的Naive-Bayes神经网络图像分类器(NBNN)

![CSDN_1579444376630(1)](C:\Users\1\Desktop\CSDN_1579444376630(1).jpg)

![1579446964703](C:\Users\1\AppData\Roaming\Typora\typora-user-images\1579446964703.png)





#### **结合几种类型的描述符:**

**最近的图像分类方法表明，在一个分类器中结合几种类型的描述符可以显著提高分类性能。**

#### **计算复杂度和运行时:**

**我们使用[23]的高效近似-r-最近邻算法和KD-树实现。**神经网络搜索的预期时间与存储在KD-tree[1]中的元素数量成对数。

## 第五部分

**5.1结果和实验**

用一个描述符类型(SIFT)和5个描述符类型的组合来测试我们的NBNN算法:



**5.2实验**

**5.3.量化的影响&图像到图像的距离。**